{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c459229-c7c2-43d7-bf32-f7faf09156e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from catboost import CatBoostClassifier\n",
    "import h3\n",
    "import shapely\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddca3a2-eaf6-4993-a976-3dadf3708d62",
   "metadata": {},
   "source": [
    "## Загружаем трейн данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb997b6-3786-4306-9aeb-605ce387c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"..\",\"data\")\n",
    "random_state=20240225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2ecc2-4fe7-4829-966a-67ccc5db93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_parquet(data_dir / \"transactions.parquet\")\n",
    "# Заполняем std = 0 где одна транзакция\n",
    "transactions_df['std'].fillna(0, inplace=True)\n",
    "# transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479cbaa9-7133-4441-bdaa-abc309c2ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.read_parquet(data_dir / \"target.parquet\")\n",
    "# target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d708b7a-d5d6-45fb-bc1f-f1507da0c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем список всех доступных Hexes\n",
    "# список всех 1658 геолокаций, где возможно снятие наличности, нужно разметить эти локации в решении\n",
    "# Есть 3 локации, по которым нет транзакций set(hexses_target).difference(transactions_df.h3_09)\n",
    "with open(data_dir / \"hexses_target.lst\", \"r\") as file:\n",
    "    file_contents = file.read()\n",
    "hexses_target = file_contents[:-1].split(\"\\n\") # remove /n\n",
    "assert set(hexses_target)==set(target_df.h3_09)\n",
    "\n",
    "# Cписок всех 8154 геолокаций h3_09 из transactions_df\n",
    "with open(data_dir / \"hexses_data.lst\", \"r\") as file:\n",
    "    file_contents = file.read()\n",
    "hexses_data = file_contents[:-1].split(\"\\n\")\n",
    "\n",
    "all_hexses = list(set(hexses_target) | set(hexses_data))\n",
    "# all_hexses = pd.DataFrame({\"h3_09\":all_hexses})\n",
    "all_hexses = gpd.GeoDataFrame({\"h3_09\":all_hexses})\n",
    "all_hexses[\"geometry\"] = all_hexses[\"h3_09\"].apply(lambda x: shapely.geometry.Polygon(h3.h3_to_geo_boundary(x, geo_json=True)))\n",
    "all_hexses[['lat', 'lon']] = all_hexses['h3_09'].apply(lambda x: pd.Series(h3.h3_to_geo(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddffc2-f65f-49f2-a5a5-5677bee6793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target.groupby(by='h3_09').size().hist(bins=30)\n",
    "# target.groupby(by='customer_id').size().hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0edad8-50b7-4164-8607-13c15054fbba",
   "metadata": {},
   "source": [
    "# Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23a050-079d-494b-8436-4c14383bc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransform:\n",
    "    def __init__(self, filepath=None):\n",
    "        if filepath:\n",
    "            self.load(filepath)\n",
    "        pass\n",
    "    \n",
    "    def fit(self, transactions):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, transactions):\n",
    "        return transactions\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        pass\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        pass\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self, transform=None, filepath=None):\n",
    "        self.transform = transform if transform else BaseTransform()\n",
    "        if filepath:\n",
    "            self.load(filepath)\n",
    "        pass\n",
    "   \n",
    "    def fit(self, transactions, target):\n",
    "        pass\n",
    "        \n",
    "    def predict(self, transactions):\n",
    "        pass\n",
    "\n",
    "    def score(self, transactions, target, return_raw_score=False):\n",
    "        labels = (\n",
    "            target\n",
    "            .assign(customer_id = lambda x: x.customer_id.astype(int))\n",
    "            .pipe(lambda x: pd.pivot(x.assign(v = 1.), index='customer_id', columns='h3_09', values='v'))\n",
    "            .pipe(lambda x: pd.concat([x, pd.DataFrame({col: np.zeros(len(x)) for col in hexses_target if col not in x.columns}, index=x.index)], axis=1))\n",
    "            .pipe(lambda x: x.reindex(sorted(x.columns), axis=1)) # Сортируем столбцы\n",
    "            .sort_values(by='customer_id')\n",
    "            .fillna(0)).values\n",
    "        \n",
    "        predict = (\n",
    "            self.predict(transactions)\n",
    "            .pipe(lambda x: x.reindex(sorted(x.columns), axis=1)) # Сортируем столбцы\n",
    "            .sort_values(by='customer_id')\n",
    "            .set_index(\"customer_id\")\n",
    "        )\n",
    "        row_score = (\n",
    "            -np.log(predict.clip(1e-8, 1 - 1e-8)) * labels\n",
    "            -np.log(1-predict.clip(1e-8, 1 - 1e-8)) * (1 - labels)\n",
    "        ).sum(axis=1)\n",
    "        col_score = (\n",
    "            -np.log(predict.clip(1e-8, 1 - 1e-8)) * labels\n",
    "            -np.log(1-predict.clip(1e-8, 1 - 1e-8)) * (1 - labels)\n",
    "        ).sum(axis=0)\n",
    "\n",
    "        score = row_score.mean()\n",
    "        if return_raw_score:\n",
    "            return score, row_score, col_score, predict\n",
    "        else: \n",
    "            return score\n",
    "\n",
    "    def save(self, filepath: Path):\n",
    "        pass\n",
    "        \n",
    "    def load(self, filepath: Path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfcdbd-3a08-427a-a693-70f5b4ae1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавить фичи: сумма всех трат клиента, кол-во транзакций клиента, кол-во хексов где были транзакции клиента, \n",
    "# Округлить 'sum', 'avg', 'min', 'max' до целых \n",
    "class SimpleFeaturesTransform(BaseTransform):\n",
    "    def __init__(self, filepath=None):\n",
    "        if filepath:\n",
    "            self.load(filepath)\n",
    "        pass\n",
    "    \n",
    "    def fit(self, transactions):\n",
    "        self.mcc_codes = set(transactions.mcc_code.unique())\n",
    "        self.datetime_ids = set(transactions.datetime_id.unique())\n",
    "        self.hexes = set(hexses_target)\n",
    "        chunk = dict()\n",
    "        for mcc_code in self.mcc_codes:\n",
    "            chunk[f\"mcc_{ mcc_code }\"] = 0\n",
    "        for datetime_id in self.datetime_ids:\n",
    "            chunk[f\"dt_{ datetime_id }\"] = 0\n",
    "        for h3_09 in self.hexes:\n",
    "            chunk[f\"hex_{ h3_09 }_count\"] = 0\n",
    "        for h3_09 in self.hexes:\n",
    "            chunk[f\"hex_{ h3_09 }_sum\"] = 0\n",
    "        self.template = chunk\n",
    "    \n",
    "    def transform(self, transactions):\n",
    "        features = []\n",
    "        row_labels = []\n",
    "        gb = transactions.groupby(by=\"customer_id\")\n",
    "        for customer_id, group in gb:\n",
    "            row_labels.append(customer_id)\n",
    "            chunk = self.template.copy()\n",
    "            chunk[f\"max_median\"] = group['max'].median()\n",
    "            chunk[f\"min_median\"] = group['min'].median()\n",
    "            chunk[f\"avg_median\"] = group['avg'].median()\n",
    "            chunk[f\"full_sum\"] = group['sum'].median()\n",
    "            # В каких мсс кодах были покупки\n",
    "            for mcc_code, subgroup in group.groupby(by='mcc_code'):\n",
    "                if mcc_code in self.mcc_codes:\n",
    "                    chunk[f\"mcc_{ mcc_code }\"] = subgroup[\"count_distinct\"].sum()/group[\"count_distinct\"].sum() # тут было .size\n",
    "            # В какое время суток были покупки\n",
    "            for datetime_id, subgroup in group.groupby(by='datetime_id'):\n",
    "                if datetime_id in self.datetime_ids:\n",
    "                    chunk[f\"dt_{ datetime_id }\"] = subgroup[\"count_distinct\"].sum()/group[\"count_distinct\"].sum()\n",
    "            # В каких локациях были покупки - детализация по количеству покупок\n",
    "            for h3_09, subgroup in group.groupby(by='h3_09'):\n",
    "                if h3_09 in self.hexes:\n",
    "                    chunk[f\"hex_{ h3_09 }_count\"] = subgroup[\"count_distinct\"].sum()/group[\"count_distinct\"].sum()\n",
    "            # В каких локациях были покупки - детализация по сумме трат (удалить, не помогает)\n",
    "            for h3_09, subgroup in group.groupby(by='h3_09'):\n",
    "                if h3_09 in self.hexes:\n",
    "                    chunk[f\"hex_{ h3_09 }_sum\"] = subgroup[\"sum\"].sum()/group[\"sum\"].sum()\n",
    "            features.append(chunk)\n",
    "        return pd.DataFrame(features, index=row_labels).pipe(lambda x: x.reindex(sorted(x.columns), axis=1)) # Сортируем столбцы\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        pass\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        pass\n",
    "\n",
    "class CBModel(BaseModel):\n",
    "    def __init__(self, transform=None, filepath=None):\n",
    "        super().__init__(transform, filepath)\n",
    "        self.models = dict()\n",
    "        \n",
    "    def fit(self, transactions, target):\n",
    "        labels = (\n",
    "            target\n",
    "            .assign(customer_id = lambda x: x.customer_id.astype(int))\n",
    "            .pipe(lambda x: pd.pivot(x.assign(v = 1.), index='customer_id', columns='h3_09', values='v'))\n",
    "            .pipe(lambda x: pd.concat([x, pd.DataFrame({col: np.zeros(len(x)) for col in hexses_target if col not in x.columns}, index=x.index)], axis=1)) # Добавляем недостающие локации банкоматов, таких нет, но на всякий случай\n",
    "            .pipe(lambda x: x.reindex(sorted(x.columns), axis=1)) # Сортируем столбцы\n",
    "            .sort_values(by='customer_id')\n",
    "            .fillna(0))\n",
    "        self.means = dict(pd.DataFrame({\"target\": labels.mean()}).target)\n",
    "\n",
    "        self.transform.fit(transactions)\n",
    "        X = self.transform.transform(transactions)\n",
    "        for col in tqdm(labels.columns, desc=\"Train models\"):\n",
    "            y = labels[col]\n",
    "            if sum(y) > 5:\n",
    "                model = CatBoostClassifier(iterations=5, random_state=random_state, verbose=0, boosting_type=\"Plain\", allow_writing_files=False)\n",
    "                model.fit(X, y)\n",
    "                self.models[col] = model\n",
    "        \n",
    "    def predict(self, transactions):\n",
    "        X = self.transform.transform(transactions)\n",
    "        submit = pd.DataFrame({\"customer_id\": X.index})\n",
    "        for col in tqdm(hexses_target, desc=\"Inference models\"):\n",
    "            if col in self.models:\n",
    "                model = self.models[col]\n",
    "                submit[col] = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                submit[col] =  [self.means[col]] * len(X)\n",
    "                submit = submit.copy()\n",
    "        return submit\n",
    "\n",
    "    def save(self, filepath: Path):\n",
    "        pass\n",
    "        \n",
    "    def load(self, filepath: Path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa752811-a7cf-49ec-a572-18f3bceadfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для ускорения ресерча - предварительно оптимизируем скор на урезанных данных (25% от всех клиентов).\n",
    "# Если изменение привело к увеличению скора на этой выборке - проверяем эффект на полных данных.\n",
    "fast_mode = True\n",
    "\n",
    "if fast_mode:\n",
    "    num_quantiles = 10\n",
    "    n_splits=4\n",
    "    customers = (\n",
    "        pd.DataFrame({\"cnt\": target_df.groupby(by='customer_id').size()})\n",
    "        .pipe(lambda x: x.assign(bin=pd.qcut(x.cnt, num_quantiles, duplicates='drop')))\n",
    "    )\n",
    "    customer_idxs = customers.index.values\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    groups = pd.Categorical(customers.bin).codes\n",
    "    ids_train, ids_test = next(kfold.split(groups, groups))\n",
    "    customers_small_list = customers.iloc[ids_test].index.values\n",
    "    \n",
    "    target_df = target_df[target_df.customer_id.isin(customers_small_list)]\n",
    "    transactions_df = transactions_df[transactions_df.customer_id.isin(customers_small_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf40d9e-c256-43ff-9573-ccd5be279d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "preds = []\n",
    "row_scores = []\n",
    "col_scores = []\n",
    "model = CBModel(transform = SimpleFeaturesTransform())\n",
    "num_quantiles = 10\n",
    "n_splits=3\n",
    "customers = (\n",
    "    pd.DataFrame({\"cnt\": target_df.groupby(by='customer_id').size()})\n",
    "    .pipe(lambda x: x.assign(bin=pd.qcut(x.cnt, num_quantiles, duplicates='drop')))\n",
    ")\n",
    "customer_idxs = customers.index.values\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "groups = pd.Categorical(customers.bin).codes\n",
    "for fold, (ids_train, ids_test) in enumerate(kfold.split(groups, groups)):\n",
    "    customers_train = customers.iloc[ids_train].index.values\n",
    "    customers_test = customers.iloc[ids_test].index.values\n",
    "    \n",
    "    transactions_train = transactions_df.loc[transactions_df.customer_id.isin(customers_train)]\n",
    "    target_train = target_df.loc[target_df.customer_id.isin(customers_train)]\n",
    "    transactions_test = transactions_df.loc[transactions_df.customer_id.isin(customers_test)]\n",
    "    target_test = target_df.loc[target_df.customer_id.isin(customers_test)]    \n",
    "    \n",
    "    model.fit(transactions_train, target_train)\n",
    "\n",
    "    score, row_score, col_score, pred = model.score(transactions_test, target_test, return_raw_score=True)\n",
    "    col_score = col_score.to_frame()\n",
    "    col_score[\"fold\"] = fold\n",
    "    print()\n",
    "    print(np.sort(transactions_df.customer_id.unique()), np.sort(transactions_train.customer_id.unique()), np.sort(transactions_test.customer_id.unique()))\n",
    "    print(transactions_df.customer_id.unique().shape, transactions_train.customer_id.unique().shape, transactions_test.customer_id.unique().shape)\n",
    "    scores.append(score)\n",
    "    row_scores.append(row_score)\n",
    "    col_scores.append(col_score)\n",
    "    preds.append(pred)\n",
    "    print(score)\n",
    "preds = pd.concat(preds)\n",
    "row_scores = pd.concat(row_scores).reset_index()\n",
    "row_scores.columns = [\"customer_id\", \"score\"]\n",
    "col_scores = pd.concat(col_scores).reset_index()\n",
    "col_scores.columns = [\"h3_09\", \"score\", \"fold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c5630-96d3-4867-8538-5fb5c42c8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скоры на первой итерации кросс-валидации:\n",
    "# 11.017292698892238 56:24 boosting_type=\"Ordered\" iterations=5\n",
    "# 12.076262937269291 43:40 boosting_type=\"Ordered\" iterations=3\n",
    "# 10.874190489487992 10:14 boosting_type=\"Plain\" iterations=5    customers_small_list: 12.324778899109338 11.89429185482667 11.637939590418044\n",
    "# 12.324067891640993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb9664-cbdb-473d-aa59-d2d068792c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not fast_mode\n",
    "model_name = \"catboost_plain_5iter\"\n",
    "model_dir = Path(\"..\", \"models\", model_name)\n",
    "model_dir.mkdir(exist_ok=False, parents=True)\n",
    "row_scores.to_csv(model_dir / f\"row_scores.csv\", index=False)\n",
    "col_scores.to_csv(model_dir / f\"col_scores.csv\", index=False)\n",
    "preds.reset_index().to_parquet(model_dir / f\"preds.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf603533-1e38-45c1-8d37-ce618d20881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = SimpleFeaturesTransform()\n",
    "t.fit(transactions_train.iloc[:5000])\n",
    "X = t.transform(transactions_train.iloc[:5000])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdabc437-a363-4fdc-a5d8-eddf6592f14b",
   "metadata": {},
   "source": [
    "# На подумоть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf71383-5fb9-4869-a1b0-77d766846a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как-то использовать информацию из соседних хексов - соседние хексы определяем по расстоянию между хексами\n",
    "all_hexses[\"centroid\"] = all_hexses.centroid\n",
    "all_hexses = all_hexses.set_index(\"h3_09\")\n",
    "\n",
    "target_hex = all_hexses.query(\"index=='8911aa7abd7ffff'\")[\"centroid\"].iloc[0]\n",
    "all_hexses[\"centroid\"].distance(target_hex).sort_values().iloc[:5].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d7ab0-30a9-41fc-82b5-173e1527bb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
